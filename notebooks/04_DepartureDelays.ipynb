{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5398e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90aae557",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"SparkSQLdepartures\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f89b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = \"C:/Users/sean.cornillie/Education/LearningSparkV2/Spark_Dev/datasets/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20b2c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Source our csv and then create a temp table so we can execute sql code\n",
    "df = (spark.read.format(\"csv\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .option(\"header\",\"true\")\n",
    "         .load(csv))\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39dceaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|date   |delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|6    |602     |ABE   |ATL        |\n",
      "|1020600|-8   |369     |ABE   |DTW        |\n",
      "|1021245|-2   |602     |ABE   |ATL        |\n",
      "|1020605|-4   |602     |ABE   |ATL        |\n",
      "|1031245|-4   |602     |ABE   |ATL        |\n",
      "|1030605|0    |602     |ABE   |ATL        |\n",
      "|1041243|10   |602     |ABE   |ATL        |\n",
      "|1040605|28   |602     |ABE   |ATL        |\n",
      "|1051245|88   |602     |ABE   |ATL        |\n",
      "|1050605|9    |602     |ABE   |ATL        |\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff74a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Basic SQL example. Functions just like reqular query would.\n",
    "spark.sql(\"\"\"SELECT distance\n",
    "                ,origin\n",
    "                ,destination\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE 1=1\n",
    "                 AND distance > 1000\n",
    "             ORDER BY distance desc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7752b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### More basic SQL querying.\n",
    "spark.sql(\"\"\"SELECT date\n",
    "                ,delay\n",
    "                ,origin\n",
    "                ,destination\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE 1=1\n",
    "                 AND origin = 'SFO'\n",
    "                 AND destination = 'ORD'\n",
    "                 AND delay > 120\n",
    "             ORDER BY delay desc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982747c1",
   "metadata": {},
   "source": [
    "#### Exercise: Convert Date to readable format and find days that the SFO/ORD delays were most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6931a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----------+-------------------+\n",
      "|delay|distance|origin|destination|      departuredate|\n",
      "+-----+--------+------+-----------+-------------------+\n",
      "|    6|     602|   ABE|        ATL|1970-01-12 09:54:05|\n",
      "|   -8|     369|   ABE|        DTW|1970-01-12 12:30:00|\n",
      "|   -2|     602|   ABE|        ATL|1970-01-12 12:40:45|\n",
      "|   -4|     602|   ABE|        ATL|1970-01-12 12:30:05|\n",
      "|   -4|     602|   ABE|        ATL|1970-01-12 15:27:25|\n",
      "|    0|     602|   ABE|        ATL|1970-01-12 15:16:45|\n",
      "|   10|     602|   ABE|        ATL|1970-01-12 18:14:03|\n",
      "|   28|     602|   ABE|        ATL|1970-01-12 18:03:25|\n",
      "|   88|     602|   ABE|        ATL|1970-01-12 21:00:45|\n",
      "|    9|     602|   ABE|        ATL|1970-01-12 20:50:05|\n",
      "+-----+--------+------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Converting the date with specified 'MM/dd/yyyy' format threw errors, omitting timestamp method worked just fine (below).\n",
    "new_df = df.withColumn(\"departuredate\", to_timestamp(col(\"date\"))).drop(\"date\")\n",
    "\n",
    "new_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92435945",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then need to replace our temp table so we can run some SQL code\n",
    "new_df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00a9aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|DayOfWeek|          AvgDelay|\n",
      "+---------+------------------+\n",
      "|   Monday|23.425249169435215|\n",
      "|   Sunday|21.653465346534652|\n",
      "| Saturday| 16.79179810725552|\n",
      "|  Tuesday| 11.92972972972973|\n",
      "|   Friday|              11.5|\n",
      "| Thursday|11.494117647058824|\n",
      "|Wednesday| 9.346774193548388|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Aggregate avg delay by day of week\n",
    "spark.sql(\"\"\"SELECT date_format(departuredate, 'EEEE') as DayOfWeek\n",
    "                ,avg(delay) as AvgDelay\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE 1=1\n",
    "                 AND origin = 'SFO'\n",
    "                 AND destination = 'ORD'\n",
    "             GROUP BY DayOfWeek\n",
    "             ORDER by AvgDelay desc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec8230dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|DayOfWeek|NumDelays|\n",
      "+---------+---------+\n",
      "| Saturday|      167|\n",
      "| Thursday|      156|\n",
      "|   Monday|      155|\n",
      "|   Sunday|      127|\n",
      "|Wednesday|      111|\n",
      "|  Tuesday|       94|\n",
      "|   Friday|       92|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Aggregated number of delays by day of week. 'EEEE' pulls the dayofweek datepart out of the timestamp.\n",
    "spark.sql(\"\"\"SELECT date_format(departuredate, 'EEEE') as DayOfWeek\n",
    "                ,count(delay) as NumDelays\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE 1=1\n",
    "                 AND origin = 'SFO'\n",
    "                 AND destination = 'ORD'\n",
    "                 AND delay > 0\n",
    "             GROUP BY DayOfWeek\n",
    "             ORDER by NumDelays desc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3145a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|DayOfWeek|NumDelays|\n",
      "+---------+---------+\n",
      "|        7|      167|\n",
      "|        5|      156|\n",
      "|        2|      155|\n",
      "|        1|      127|\n",
      "|        4|      111|\n",
      "|        3|       94|\n",
      "|        6|       92|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Alternatively use spark built in function 'dayofweek' instead of converting the date.\n",
    "spark.sql(\"\"\"SELECT dayofweek(departuredate) as DayOfWeek\n",
    "                ,count(delay) as NumDelays\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE 1=1\n",
    "                 AND origin = 'SFO'\n",
    "                 AND destination = 'ORD'\n",
    "                 AND delay > 0\n",
    "             GROUP BY DayOfWeek\n",
    "             ORDER by NumDelays desc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0ec21",
   "metadata": {},
   "source": [
    "#### More complicated case when example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6620f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay\n",
    "                ,origin\n",
    "                ,destination\n",
    "                ,CASE\n",
    "                    WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                    WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "                    WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "                    WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "                    WHEN delay = 0 THEN 'No Delays'\n",
    "                    ELSE 'Early'\n",
    "                END as Flight_Delays\n",
    "             FROM us_delay_flights_tbl\n",
    "             ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9f1b1",
   "metadata": {},
   "source": [
    "#### Exercise: Convert a few of the SQL queries above to use the DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84d7aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"\"\"SELECT distance\n",
    "#                ,origin\n",
    "#                ,destination\n",
    "#             FROM us_delay_flights_tbl\n",
    "#             WHERE 1=1\n",
    "#                 AND distance > 1000\n",
    "#             ORDER BY distance desc\"\"\").show(10)\n",
    "\n",
    "(new_df\n",
    "    .select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .sort(desc(\"distance\"))\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8957410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----------+\n",
      "|      departuredate|origin|destination|\n",
      "+-------------------+------+-----------+\n",
      "|1970-01-12 09:56:50|   SFO|        ORD|\n",
      "|1970-01-12 12:58:50|   SFO|        ORD|\n",
      "|1970-01-12 12:43:30|   SFO|        ORD|\n",
      "|1970-01-13 10:56:50|   SFO|        ORD|\n",
      "|1970-01-14 11:48:45|   SFO|        ORD|\n",
      "|1970-01-15 01:45:10|   SFO|        ORD|\n",
      "|1970-01-15 18:36:40|   SFO|        ORD|\n",
      "|1970-01-12 09:53:57|   SFO|        ORD|\n",
      "|1970-01-12 15:44:18|   SFO|        ORD|\n",
      "|1970-01-12 15:38:40|   SFO|        ORD|\n",
      "+-------------------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"\"\"SELECT date\n",
    "#                ,delay\n",
    "#                ,origin\n",
    "#                ,destination\n",
    "#             FROM us_delay_flights_tbl\n",
    "#             WHERE 1=1\n",
    "#                 AND origin = 'SFO'\n",
    "#                 AND destination = 'ORD'\n",
    "#                 AND delay > 120\n",
    "#             ORDER BY delay desc\"\"\").show(10)\n",
    "\n",
    "(new_df\n",
    "    .select(\"departuredate\", \"origin\", \"destination\")\n",
    "    .where(col(\"origin\") == \"SFO\")\n",
    "    .where(col(\"destination\") == 'ORD')\n",
    "    .where(col(\"delay\") > 120)\n",
    "    .show(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
